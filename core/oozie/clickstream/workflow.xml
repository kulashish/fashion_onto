<workflow-app xmlns="uri:oozie:workflow:0.5" name="dap-clickstream-wf">
    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${queueName}</value>
            </property>
        </configuration>
    </global>

    <start to='clickstreamYesterdaySession' />

    <action name="clickstreamYesterdaySession">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>run.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>clickstreamYesterdaySession</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/run.pl#run.pl</file>
            <capture-output/>
        </shell>
        <ok to="clickstreamSurf3Variable"/>
        <error to="fail"/>
    </action>

    <action name="clickstreamSurf3Variable">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>run.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>clickstreamSurf3Variable</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/run.pl#run.pl</file>
            <capture-output/>
        </shell>
        <ok to="customerDeviceMapping"/>
        <error to="fail"/>
    </action>

    <action name="customerDeviceMapping">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>run.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>customerDeviceMapping</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/run.pl#run.pl</file>
            <capture-output/>
        </shell>
        <ok to="afterClickstreamVars"/>
        <error to="fail"/>
    </action>

    <fork name="afterClickstreamVars">
        <path start="pushSurfCampaign" />
        <path start="dcfFeedGenerate" />
        <path start="pricingSKUData" />
    </fork>

    <action name="pushSurfCampaign">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>run.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>pushSurfCampaign</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/run.pl#run.pl</file>
            <capture-output/>
        </shell>
        <ok to="endJoin"/>
        <error to="fail"/>
    </action>

    <action name="dcfFeedGenerate">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>run.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>dcfFeedGenerate</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/run.pl#run.pl</file>
            <capture-output/>
        </shell>
        <ok to="dcfFeedFtpUpload"/>
        <error to="fail"/>
    </action>

    <action name="dcfFeedFtpUpload">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>ftp_upload.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>dcf_feed</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/ftp_upload.pl#ftp_upload.pl</file>
            <capture-output/>
        </shell>
        <ok to="endJoin"/>
        <error to="fail"/>
    </action>

    <action name="pricingSKUData">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>run.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>pricingSKUData</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/run.pl#run.pl</file>
            <capture-output/>
        </shell>
        <ok to="pricingSKUDataFtpUpload"/>
        <error to="fail"/>
    </action>

    <action name="pricingSKUDataFtpUpload">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>ftp_upload.pl</exec>
            <argument>-t</argument>
            <argument>${jobEnv}</argument>
            <argument>-c</argument>
            <argument>pricing_sku_data</argument>
            <env-var>YARN_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>HADOOP_USER_NAME=${wf:user()}</env-var>
            <file>${scriptHome}/ftp_upload.pl#ftp_upload.pl</file>
            <capture-output/>
        </shell>
        <ok to="endJoin"/>
        <error to="fail"/>
    </action>

    <join name="endJoin" to="end"/>

    <kill name="fail">
        <message>Workflow failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>

    <end name="end" />

</workflow-app>